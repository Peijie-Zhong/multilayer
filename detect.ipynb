{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "803345a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import algorithms\n",
    "from importlib import reload\n",
    "reload(algorithms)\n",
    "from algorithms import *\n",
    "\n",
    "filename = \"test\"\n",
    "input_file = f'sync_data/{filename}.csv'\n",
    "output_file = f'output/{filename}_communities.csv'\n",
    "\n",
    "infomap_communities(input_file, output_file, jsd_relax_rate=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "615050e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8641253736288659, 0.9336986067992395, 0.8876319653726334, 0.8943528197602778, 0.9082457389242938]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8976109008970623"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import measurement\n",
    "reload(measurement)\n",
    "from measurement import *\n",
    "\n",
    "compute_layerwise_ami_avg(\"test\",\"sync_data\",output_dir=\"output\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2e2834f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings for layer 0 -> out/embeddings_layer_0.csv\n",
      "Saved embeddings for layer 1 -> out/embeddings_layer_1.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings for layer 2 -> out/embeddings_layer_2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved embeddings for layer 3 -> out/embeddings_layer_3.csv\n",
      "Saved embeddings for layer 4 -> out/embeddings_layer_4.csv\n",
      "GW distance(0, 0) = 0.000000\n",
      "GW distance(0, 1) = 0.674594\n",
      "GW distance(0, 2) = 0.677660\n",
      "GW distance(0, 3) = 0.641303\n",
      "GW distance(0, 4) = 0.620872\n",
      "GW distance(1, 1) = 0.000000\n",
      "GW distance(1, 2) = 0.319740\n",
      "GW distance(1, 3) = 0.320457\n",
      "GW distance(1, 4) = 0.343885\n",
      "GW distance(2, 2) = 0.000000\n",
      "GW distance(2, 3) = 0.354276\n",
      "GW distance(2, 4) = 0.344384\n",
      "GW distance(3, 3) = 0.000000\n",
      "GW distance(3, 4) = 0.330284\n",
      "GW distance(4, 4) = 0.000000\n",
      "Saved GW distance matrix -> out/gw_distances.csv\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Build a multilayer network from a CSV with columns:\n",
    "    u,v,layer,u_label,v_label\n",
    "Compute Node2Vec embeddings per layer, then compute pairwise\n",
    "Gromov–Wasserstein (GW) distances between layers using POT.\n",
    "\n",
    "No argparse version: set your parameters in the CONFIG section below\n",
    "and run the script directly (python multilayer_node2vec_gw.py).\n",
    "\n",
    "Dependencies (install if needed):\n",
    "    pip install pandas networkx node2vec POT gensim\n",
    "\n",
    "Outputs:\n",
    "  - out/embeddings_layer_<layer>.csv : per-layer embeddings\n",
    "  - out/gw_distances.csv             : symmetric matrix of GW distances between layers\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# =====================\n",
    "# CONFIG — EDIT HERE\n",
    "# =====================\n",
    "CSV_PATH = \"sync_data/test.csv\"          # 输入 CSV 路径\n",
    "OUT_DIR = \"out\"                           # 输出目录\n",
    "\n",
    "# node2vec 超参数\n",
    "DIMENSIONS = 64\n",
    "WALK_LENGTH = 80\n",
    "NUM_WALKS = 10\n",
    "P = 1.0\n",
    "Q = 1.0\n",
    "WINDOW = 10\n",
    "WORKERS = 1\n",
    "SEED = 42\n",
    "\n",
    "# GW 距离使用的“空间”\n",
    "#   - 'embedding' : 用每层的 node2vec 向量计算层内两两节点距离矩阵，再做 GW（你想要的）\n",
    "#   - 'shortest_path' : 用图的最短路距离做 GW（原实现，作为可选）\n",
    "GW_SPACE = 'embedding'   # 'embedding' 或 'shortest_path'\n",
    "GW_EMB_METRIC = 'euclidean'  # 'euclidean' 或 'cosine'\n",
    "# =====================\n",
    "# IMPLEMENTATION\n",
    "# =====================\n",
    "\n",
    "# node2vec implementation\n",
    "try:\n",
    "    from node2vec import Node2Vec\n",
    "except Exception as e:\n",
    "    raise SystemExit(\n",
    "        \"The 'node2vec' package is required. Install via: pip install node2vec gensim\"\n",
    "        f\"Import error: {e}\"\n",
    "    )\n",
    "\n",
    "# POT for GW distance\n",
    "try:\n",
    "    from ot.gromov import gromov_wasserstein2\n",
    "except Exception as e:\n",
    "    raise SystemExit(\n",
    "        \"The 'POT' package is required. Install via: pip install POT\"\n",
    "        f\"Import error: {e}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def read_edges(csv_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    required = {\"u\", \"v\", \"layer\"}\n",
    "    missing = required - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"CSV missing required columns: {sorted(missing)}\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def build_layer_graphs(df: pd.DataFrame) -> Dict[str, nx.Graph]:\n",
    "    \"\"\"Return dict: layer -> NetworkX Graph built from edges in that layer.\"\"\"\n",
    "    graphs: Dict[str, nx.Graph] = {}\n",
    "    for layer, gdf in df.groupby(\"layer\"):\n",
    "        G = nx.Graph()\n",
    "        edges = [(str(u), str(v)) for u, v in zip(gdf[\"u\"], gdf[\"v\"])]\n",
    "        G.add_edges_from(edges)\n",
    "        for col in [\"u\", \"v\"]:\n",
    "            for n in gdf[col].astype(str).unique():\n",
    "                if n not in G:\n",
    "                    G.add_node(n)\n",
    "        graphs[str(layer)] = G\n",
    "    return graphs\n",
    "\n",
    "\n",
    "def compute_node2vec_embeddings(\n",
    "    G: nx.Graph,\n",
    "    dimensions: int = 64,\n",
    "    walk_length: int = 80,\n",
    "    num_walks: int = 10,\n",
    "    p: float = 1.0,\n",
    "    q: float = 1.0,\n",
    "    window: int = 10,\n",
    "    workers: int = 4,\n",
    "    seed: int = 42,\n",
    ") -> pd.DataFrame:\n",
    "    if len(G) == 0:\n",
    "        raise ValueError(\"Graph has no nodes.\")\n",
    "\n",
    "    def _fit(n_jobs: int):\n",
    "        n2v = Node2Vec(\n",
    "            G,\n",
    "            dimensions=dimensions,\n",
    "            walk_length=walk_length,\n",
    "            num_walks=num_walks,\n",
    "            p=p,\n",
    "            q=q,\n",
    "            workers=n_jobs,   # used both for walk generation (joblib) and gensim training threads\n",
    "            seed=seed,\n",
    "            quiet=True,\n",
    "        )\n",
    "        return n2v.fit(window=window, min_count=1, batch_words=4)\n",
    "\n",
    "    try:\n",
    "        model = _fit(workers)\n",
    "    except Exception as e:\n",
    "        # Fallback for environments where joblib multiprocessing breaks (e.g., numpy/joblib mismatch)\n",
    "        print(f\"[WARN] node2vec parallel walk generation failed with workers={workers} due to: {e}\"\n",
    "              f\"       Falling back to single-process mode (workers=1). Consider upgrading numpy/joblib.\")\n",
    "        model = _fit(1)\n",
    "\n",
    "    nodes = [str(n) for n in G.nodes()]\n",
    "    vecs = np.vstack([model.wv[n] for n in nodes])\n",
    "    cols = [f\"f{i}\" for i in range(dimensions)]\n",
    "    emb = pd.DataFrame(vecs, index=nodes, columns=cols)\n",
    "    emb.index.name = \"node\"\n",
    "    return emb\n",
    "\n",
    "\n",
    "def _all_pairs_shortest_path_matrix(G: nx.Graph, nodes: List[str]) -> np.ndarray:\n",
    "    spl = dict(nx.all_pairs_shortest_path_length(G))\n",
    "    finite_dists = []\n",
    "    for i in nodes:\n",
    "        di = spl.get(i, {})\n",
    "        for j, d in di.items():\n",
    "            if i != j:\n",
    "                finite_dists.append(d)\n",
    "    max_finite = max(finite_dists) if finite_dists else 0\n",
    "    penalty = (max_finite + 1) if max_finite > 0 else 1.0\n",
    "\n",
    "    n = len(nodes)\n",
    "    C = np.zeros((n, n), dtype=float)\n",
    "    for a, i in enumerate(nodes):\n",
    "        di = spl.get(i, {})\n",
    "        for b, j in enumerate(nodes):\n",
    "            if i == j:\n",
    "                C[a, b] = 0.0\n",
    "            else:\n",
    "                C[a, b] = float(di.get(j, penalty))\n",
    "    return C\n",
    "\n",
    "\n",
    "def _pairwise_distance_matrix_from_embeddings(emb: pd.DataFrame, metric: str = 'euclidean') -> np.ndarray:\n",
    "    \"\"\"Compute layer-internal pairwise distances from node embeddings.\n",
    "    emb: DataFrame indexed by node id, columns are features.\n",
    "    metric: 'euclidean' or 'cosine' (converted to distance: 1 - cosine).\n",
    "    \"\"\"\n",
    "    X = emb.values.astype(float)\n",
    "    if metric == 'euclidean':\n",
    "        # (x - y)^2 = ||x||^2 + ||y||^2 - 2 x·y\n",
    "        sq_norms = np.sum(X * X, axis=1, keepdims=True)\n",
    "        # broadcasting to get squared distances\n",
    "        D2 = sq_norms + sq_norms.T - 2 * (X @ X.T)\n",
    "        np.maximum(D2, 0.0, out=D2)\n",
    "        return np.sqrt(D2, dtype=float)\n",
    "    elif metric == 'cosine':\n",
    "        # normalize rows\n",
    "        norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "        norms[norms == 0] = 1.0\n",
    "        Xn = X / norms\n",
    "        S = Xn @ Xn.T  # cosine similarity in [-1,1]\n",
    "        # ensure numerical range\n",
    "        S = np.clip(S, -1.0, 1.0)\n",
    "        return 1.0 - S\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown metric '{metric}'. Use 'euclidean' or 'cosine'.\")\n",
    "\n",
    "\n",
    "def gw_distance_between_layers_from_graphs(G1: nx.Graph, G2: nx.Graph) -> float:\n",
    "    nodes1 = [str(n) for n in G1.nodes()]\n",
    "    nodes2 = [str(n) for n in G2.nodes()]\n",
    "    C1 = _all_pairs_shortest_path_matrix(G1, nodes1)\n",
    "    C2 = _all_pairs_shortest_path_matrix(G2, nodes2)\n",
    "    p = np.ones(len(nodes1)) / max(len(nodes1), 1)\n",
    "    q = np.ones(len(nodes2)) / max(len(nodes2), 1)\n",
    "    gw2 = gromov_wasserstein2(C1, C2, p, q, loss_function=\"square_loss\")\n",
    "    return float(np.sqrt(max(gw2, 0.0)))\n",
    "\n",
    "\n",
    "def gw_distance_between_layers_from_embeddings(emb1: pd.DataFrame, emb2: pd.DataFrame, metric: str = 'euclidean') -> float:\n",
    "    # Keep node ordering stable but arbitrary\n",
    "    emb1 = emb1.sort_index()\n",
    "    emb2 = emb2.sort_index()\n",
    "    C1 = _pairwise_distance_matrix_from_embeddings(emb1, metric=metric)\n",
    "    C2 = _pairwise_distance_matrix_from_embeddings(emb2, metric=metric)\n",
    "    n1, n2 = C1.shape[0], C2.shape[0]\n",
    "    p = np.ones(n1) / max(n1, 1)\n",
    "    q = np.ones(n2) / max(n2, 1)\n",
    "    gw2 = gromov_wasserstein2(C1, C2, p, q, loss_function=\"square_loss\")\n",
    "    return float(np.sqrt(max(gw2, 0.0)))\n",
    "\n",
    "\n",
    "\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "df = read_edges(CSV_PATH)\n",
    "graphs = build_layer_graphs(df)\n",
    "\n",
    "# Embeddings per layer (store in dict)\n",
    "layer_names = sorted(graphs.keys(), key=lambda x: (str(x)))\n",
    "embeddings: Dict[str, pd.DataFrame] = {}\n",
    "for layer in layer_names:\n",
    "    emb = compute_node2vec_embeddings(\n",
    "        graphs[layer],\n",
    "        dimensions=DIMENSIONS,\n",
    "        walk_length=WALK_LENGTH,\n",
    "        num_walks=NUM_WALKS,\n",
    "        p=P,\n",
    "        q=Q,\n",
    "        window=WINDOW,\n",
    "        workers=WORKERS,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    embeddings[layer] = emb\n",
    "    emb_path = os.path.join(OUT_DIR, f\"embeddings_layer_{layer}.csv\")\n",
    "    emb.to_csv(emb_path)\n",
    "    print(f\"Saved embeddings for layer {layer} -> {emb_path}\")\n",
    "\n",
    "# Pairwise GW distances between layers\n",
    "L = len(layer_names)\n",
    "D = np.zeros((L, L), dtype=float)\n",
    "for i in range(L):\n",
    "    for j in range(i, L):\n",
    "        if GW_SPACE == 'embedding':\n",
    "            d = gw_distance_between_layers_from_embeddings(\n",
    "                embeddings[layer_names[i]], embeddings[layer_names[j]], metric=GW_EMB_METRIC\n",
    "            )\n",
    "        elif GW_SPACE == 'shortest_path':\n",
    "            d = gw_distance_between_layers_from_graphs(\n",
    "                graphs[layer_names[i]], graphs[layer_names[j]]\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"GW_SPACE must be 'embedding' or 'shortest_path'\")\n",
    "        D[i, j] = D[j, i] = d\n",
    "        print(f\"GW distance({layer_names[i]}, {layer_names[j]}) = {d:.6f}\")\n",
    "\n",
    "dist_df = pd.DataFrame(D, index=layer_names, columns=layer_names)\n",
    "dist_csv = os.path.join(OUT_DIR, \"gw_distances.csv\")\n",
    "dist_df.to_csv(dist_csv)\n",
    "print(f\"Saved GW distance matrix -> {dist_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9e761f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n",
      "Exception ignored in: 'gensim.models.word2vec_inner.our_dot_float'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved community assignments -> out/infomap_state_nodes.csv (1820 nodes)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# ========= CONFIG =========\n",
    "CSV_PATH = \"sync_data/test.csv\"\n",
    "# node2vec\n",
    "DIMENSIONS = 32\n",
    "WALK_LENGTH = 80\n",
    "NUM_WALKS = 10\n",
    "P = 1.0\n",
    "Q = 1.0\n",
    "WINDOW = 10\n",
    "WORKERS = 1\n",
    "SEED = 42\n",
    "# GW & interlayer linking\n",
    "GW_EMB_METRIC = \"euclidean\"       # 'euclidean' or 'cosine'\n",
    "TOP_K = 5                         # 每个源节点跨层只保留前 K 个目标\n",
    "MIN_WEIGHT = 1e-3                 # 小于阈值的跨层边丢弃\n",
    "ALPHA = 2.0                       # 层间全局缩放 Omega = exp(-ALPHA * GW)\n",
    "INTRA_EDGE_WEIGHT = 1.0           # 层内边权（CSV 若无权重，就用常数 1）\n",
    "\n",
    "# ========= deps =========\n",
    "from node2vec import Node2Vec\n",
    "from ot.gromov import gromov_wasserstein, gromov_wasserstein2\n",
    "from infomap import Infomap\n",
    "\n",
    "\n",
    "# ---------- data / graphs ----------\n",
    "def read_edges(csv_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    req = {\"u\", \"v\", \"layer\"}\n",
    "    miss = req - set(df.columns)\n",
    "    if miss:\n",
    "        raise ValueError(f\"CSV missing required columns: {sorted(miss)}\")\n",
    "    # 统一为字符串 id\n",
    "    df[\"u\"] = df[\"u\"].astype(str)\n",
    "    df[\"v\"] = df[\"v\"].astype(str)\n",
    "    df[\"layer\"] = df[\"layer\"].astype(str)\n",
    "    return df\n",
    "\n",
    "def build_layer_graphs(df: pd.DataFrame) -> Dict[str, nx.Graph]:\n",
    "    graphs: Dict[str, nx.Graph] = {}\n",
    "    for layer, gdf in df.groupby(\"layer\"):\n",
    "        G = nx.Graph()\n",
    "        if \"w\" in gdf.columns:\n",
    "            edges = [(u, v, float(w)) for u, v, w in gdf[[\"u\", \"v\", \"w\"]].itertuples(index=False)]\n",
    "            G.add_weighted_edges_from(edges)\n",
    "        else:\n",
    "            G.add_edges_from(gdf[[\"u\", \"v\"]].itertuples(index=False, name=None))\n",
    "            # 给无权图统一权重 1\n",
    "            nx.set_edge_attributes(G, INTRA_EDGE_WEIGHT, \"weight\")\n",
    "        # 确保孤立点也在图里（如果 CSV 有只出现一次的端点）\n",
    "        for col in [\"u\", \"v\"]:\n",
    "            for n in gdf[col].unique():\n",
    "                if n not in G:\n",
    "                    G.add_node(n)\n",
    "        graphs[layer] = G\n",
    "    return graphs\n",
    "\n",
    "\n",
    "# ---------- embeddings ----------\n",
    "def compute_node2vec_embeddings(\n",
    "    G: nx.Graph,\n",
    "    dimensions=64, walk_length=80, num_walks=10, p=1.0, q=1.0,\n",
    "    window=10, workers=1, seed=42\n",
    ") -> pd.DataFrame:\n",
    "    if len(G) == 0:\n",
    "        raise ValueError(\"Empty graph.\")\n",
    "    def _fit(n_jobs):\n",
    "        n2v = Node2Vec(G, dimensions=dimensions, walk_length=walk_length,\n",
    "                       num_walks=num_walks, p=p, q=q, workers=n_jobs,\n",
    "                       seed=seed, quiet=True)\n",
    "        return n2v.fit(window=window, min_count=1, batch_words=4)\n",
    "    try:\n",
    "        model = _fit(workers)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] node2vec workers={workers} failed: {e}. Fallback to workers=1.\")\n",
    "        model = _fit(1)\n",
    "    nodes = [str(n) for n in G.nodes()]\n",
    "    X = np.vstack([model.wv[n] for n in nodes])\n",
    "    emb = pd.DataFrame(X, index=nodes, columns=[f\"f{i}\" for i in range(X.shape[1])])\n",
    "    emb.index.name = \"node\"\n",
    "    return emb\n",
    "\n",
    "def pairwise_from_embeddings(emb: pd.DataFrame, metric=\"euclidean\") -> np.ndarray:\n",
    "    X = emb.values.astype(float)\n",
    "    if metric == \"euclidean\":\n",
    "        s = np.sum(X * X, axis=1, keepdims=True)\n",
    "        D2 = s + s.T - 2 * (X @ X.T)\n",
    "        np.maximum(D2, 0.0, out=D2)\n",
    "        return np.sqrt(D2, dtype=float)\n",
    "    elif metric == \"cosine\":\n",
    "        n = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "        n[n == 0] = 1.0\n",
    "        Xn = X / n\n",
    "        S = np.clip(Xn @ Xn.T, -1.0, 1.0)\n",
    "        return 1.0 - S\n",
    "    else:\n",
    "        raise ValueError(\"metric must be 'euclidean' or 'cosine'.\")\n",
    "\n",
    "def gw_coupling_and_distance(emb1: pd.DataFrame, emb2: pd.DataFrame, metric=\"euclidean\"):\n",
    "    emb1 = emb1.sort_index()\n",
    "    emb2 = emb2.sort_index()\n",
    "    nodes1 = emb1.index.to_list()\n",
    "    nodes2 = emb2.index.to_list()\n",
    "    C1 = pairwise_from_embeddings(emb1, metric=metric)\n",
    "    C2 = pairwise_from_embeddings(emb2, metric=metric)\n",
    "    n1, n2 = C1.shape[0], C2.shape[0]\n",
    "    p = np.ones(n1) / max(n1, 1)\n",
    "    q = np.ones(n2) / max(n2, 1)\n",
    "    # π（最优耦合，带熵正则版可用 ot.gromov.entropic_gromov_wasserstein）\n",
    "    pi = gromov_wasserstein(C1, C2, p, q, loss_fun=\"square_loss\")\n",
    "    # GW^2\n",
    "    gw2 = gromov_wasserstein2(C1, C2, p, q, loss_function=\"square_loss\")\n",
    "    gw = float(np.sqrt(max(gw2, 0.0)))\n",
    "    return pi, nodes1, nodes2, gw\n",
    "\n",
    "def state_id(node: str, layer: str) -> str:\n",
    "    # 选择一个不太可能出现在节点/层名里的分隔符\n",
    "    return f\"{node}|{layer}\"\n",
    "def build_infomap_with_gw(\n",
    "    graphs: Dict[str, nx.Graph],\n",
    "    embeddings: Dict[str, pd.DataFrame],\n",
    "    layer_order: List[str],\n",
    "    top_k: int = 5,\n",
    "    min_weight: float = 1e-3,\n",
    "    alpha: float = 2.0,\n",
    "    emb_metric: str = \"euclidean\",\n",
    "    intra_weight_from_attr: str = \"weight\"\n",
    ") -> Tuple[Infomap, Dict[int, Tuple[str, str]]]:\n",
    "    \"\"\"\n",
    "    构造扁平化“状态节点图”，节点是 (node, layer) 的组合映射为 int。\n",
    "    返回 (Infomap实例, id2state)，其中 id2state[i] = (node, layer)\n",
    "    \"\"\"\n",
    "\n",
    "    # ----------- 1️⃣ 建立全局状态节点映射 -----------\n",
    "    state2id = {}\n",
    "    id2state = {}\n",
    "    next_id = 1\n",
    "\n",
    "    # 遍历所有层的所有节点，创建唯一的整数ID\n",
    "    for layer in layer_order:\n",
    "        for n in graphs[layer].nodes():\n",
    "            key = (str(n), str(layer))\n",
    "            state2id[key] = next_id\n",
    "            id2state[next_id] = key\n",
    "            next_id += 1\n",
    "\n",
    "    im = Infomap(\"--two-level --silent\")\n",
    "\n",
    "    # ----------- 2️⃣ 层内边 -----------\n",
    "    for layer in layer_order:\n",
    "        G = graphs[layer]\n",
    "        for u, v, d in G.edges(data=True):\n",
    "            w = float(d.get(intra_weight_from_attr, INTRA_EDGE_WEIGHT))\n",
    "            uid = state2id[(str(u), str(layer))]\n",
    "            vid = state2id[(str(v), str(layer))]\n",
    "            im.addLink(int(uid), int(vid), float(w))\n",
    "\n",
    "        # 如果整层没有边，加极小自环，保证节点可见\n",
    "        if G.number_of_edges() == 0:\n",
    "            for n in G.nodes():\n",
    "                nid = state2id[(str(n), str(layer))]\n",
    "                im.addLink(int(nid), int(nid), 1e-12)\n",
    "\n",
    "    # ----------- 3️⃣ 层间边 (GW 耦合) -----------\n",
    "    for t in range(len(layer_order) - 1):\n",
    "        la, lb = layer_order[t], layer_order[t + 1]\n",
    "        emb_a, emb_b = embeddings[la].sort_index(), embeddings[lb].sort_index()\n",
    "        pi, nodes_a, nodes_b, gw = gw_coupling_and_distance(emb_a, emb_b, metric=emb_metric)\n",
    "\n",
    "        Omega = float(np.exp(-alpha * gw))  # 全局缩放\n",
    "        row_sums = pi.sum(axis=1, keepdims=True)\n",
    "        row_sums[row_sums == 0] = 1.0\n",
    "        P = pi / row_sums\n",
    "\n",
    "        na, nb = P.shape\n",
    "        for ia in range(na):\n",
    "            k_use = min(top_k, nb) if top_k > 0 else nb\n",
    "            idx = np.argpartition(-P[ia, :], kth=k_use - 1)[:k_use]\n",
    "            idx = idx[np.argsort(-P[ia, idx])]\n",
    "\n",
    "            src_name = nodes_a[ia]\n",
    "            src_id = state2id[(str(src_name), str(la))]\n",
    "\n",
    "            for jb in idx:\n",
    "                w = float(P[ia, jb]) * Omega\n",
    "                if w < min_weight:\n",
    "                    continue\n",
    "                dst_name = nodes_b[jb]\n",
    "                dst_id = state2id[(str(dst_name), str(lb))]\n",
    "                im.addLink(int(src_id), int(dst_id), float(w))\n",
    "\n",
    "    return im, id2state\n",
    "\n",
    "\n",
    "# ============ RUN ============\n",
    "df = read_edges(CSV_PATH)\n",
    "graphs = build_layer_graphs(df)\n",
    "layer_names = sorted(graphs.keys(), key=lambda x: x)  # 你也可以自定义顺序\n",
    "\n",
    "# 逐层嵌入\n",
    "embeddings: Dict[str, pd.DataFrame] = {}\n",
    "for layer in layer_names:\n",
    "    embeddings[layer] = compute_node2vec_embeddings(\n",
    "        graphs[layer],\n",
    "        dimensions=DIMENSIONS, walk_length=WALK_LENGTH, num_walks=NUM_WALKS,\n",
    "        p=P, q=Q, window=WINDOW, workers=WORKERS, seed=SEED\n",
    "    )\n",
    "\n",
    "    \n",
    "im, id2state = build_infomap_with_gw(\n",
    "    graphs, embeddings, layer_order=layer_names,\n",
    "    top_k=TOP_K, min_weight=MIN_WEIGHT, alpha=ALPHA, emb_metric=GW_EMB_METRIC\n",
    ")\n",
    "\n",
    "im.run()\n",
    "\n",
    "# 提取社区划分结果\n",
    "partition = {n.node_id: n.module_id for n in im.tree if n.is_leaf}\n",
    "\n",
    "# 映射回 (node, layer)\n",
    "rows = []\n",
    "for nid, com in partition.items():\n",
    "    if nid in id2state:\n",
    "        node, layer = id2state[nid]\n",
    "        rows.append((node, layer, com))\n",
    "\n",
    "assignments_df = pd.DataFrame(rows, columns=[\"node_id\", \"layer\", \"community\"])\n",
    "assignments_df.to_csv(\"out/infomap_state_nodes.csv\", index=False)\n",
    "print(f\"Saved community assignments -> out/infomap_state_nodes.csv ({len(rows)} nodes)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be119077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6052790034077675, 0.9344613973053375, 0.8789571703502499, 0.9038561123671298, 0.9164749858448414]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8478057338550652"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import measurement\n",
    "\n",
    "from measurement import *\n",
    "\n",
    "compute_layerwise_ami_avg(\"test\",\"sync_data\",output_dir=\"output\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
