{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f21a1466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "/var/folders/6r/8pf7t78x2jz69wslwjn4r2sc0000gn/T/ipykernel_84763/2502844182.py:1: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Step    1] COPT obj = 514.562386\n",
      "[Step   20] COPT obj = 423.730343\n",
      "[Step   40] COPT obj = 417.419936\n",
      "[Step   60] COPT obj = 410.964449\n",
      "[Step   80] COPT obj = 402.721435\n",
      "[Step  100] COPT obj = 389.977649\n",
      "[Step  120] COPT obj = 380.059954\n",
      "[Step  140] COPT obj = 368.885516\n",
      "[Step  160] COPT obj = 359.842705\n",
      "[Step  180] COPT obj = 352.344428\n",
      "[Step  200] COPT obj = 348.508909\n",
      "\n",
      "COPT distance: 348.5089088111093\n",
      "P shape: torch.Size([30, 25])\n",
      "Row sums (should be ~M): tensor([25.0000, 25.0000, 25.0000, 25.0000, 25.0000])\n",
      "Col sums (should be ~N): tensor([30.0000, 30.0000, 30.0000, 30.0000, 30.0000])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Minimal, runnable PyTorch implementation of the COPT distance between two graphs\n",
    "(based on the closed-form objective over P with Laplacian pseudoinverses).\n",
    "\n",
    "It optimizes the transport matrix P under nonnegativity and (row/col) sum constraints\n",
    "using a multiplicative Sinkhorn-style projection each step.\n",
    "\n",
    "Usage (example at bottom):\n",
    "- Provide adjacency matrices A_x (N x N) and A_y (M x M) as torch tensors.\n",
    "- Call copt_distance(A_x, A_y, ...). Returns (distance_value, P_opt).\n",
    "\n",
    "Notes:\n",
    "- We compute L^\\dagger by eigen-decomposition on the subspace orthogonal to constants.\n",
    "- tr(sqrt(S)) is computed as sum of sqrt of eigenvalues of symmetric PSD S.\n",
    "- For numerical stability we clamp tiny eigenvalues and add small diagonal jitter where needed.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def laplacian_from_adj(A: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Unnormalized graph Laplacian L = D - A. Assumes undirected A with nonnegative entries.\"\"\"\n",
    "    assert A.dim() == 2 and A.size(0) == A.size(1), \"A must be square\"\n",
    "    d = A.sum(dim=1)\n",
    "    L = torch.diag(d) - A\n",
    "    # ensure symmetry\n",
    "    return 0.5 * (L + L.T)\n",
    "\n",
    "\n",
    "def pinv_laplacian(L: torch.Tensor, eps: float = 1e-8, tol: float = 1e-10) -> torch.Tensor:\n",
    "    \"\"\"Moore-Penrose pseudoinverse of a graph Laplacian via eigendecomposition.\n",
    "    Drops the eigenvalue associated with the constant vector; in practice, clamps small lambdas.\n",
    "    \"\"\"\n",
    "    # Symmetrize\n",
    "    Ls = 0.5 * (L + L.T)\n",
    "    # Eigen-decomposition (symmetric/Hermitian)\n",
    "    evals, evecs = torch.linalg.eigh(Ls)\n",
    "    # Clamp tiny negatives due to numerical noise\n",
    "    evals = torch.clamp(evals, min=0.0)\n",
    "    # Build pseudoinverse spectrum: 0 for ~0 eigenvalues, 1/lambda otherwise\n",
    "    mask = evals > tol\n",
    "    inv_evals = torch.zeros_like(evals)\n",
    "    inv_evals[mask] = 1.0 / torch.clamp(evals[mask], min=eps)\n",
    "    L_pinv = (evecs * inv_evals) @ evecs.T\n",
    "    # Make perfectly symmetric\n",
    "    return 0.5 * (L_pinv + L_pinv.T)\n",
    "\n",
    "\n",
    "def sqrtm_psd(M: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:\n",
    "    \"\"\"Matrix square root for symmetric PSD matrices via eigen-decomposition.\"\"\"\n",
    "    Ms = 0.5 * (M + M.T)\n",
    "    evals, evecs = torch.linalg.eigh(Ms)\n",
    "    evals = torch.clamp(evals, min=eps)\n",
    "    sqrt_evals = torch.sqrt(evals)\n",
    "    S = (evecs * sqrt_evals) @ evecs.T\n",
    "    return 0.5 * (S + S.T)\n",
    "\n",
    "\n",
    "def trace_sqrt_psd(M: torch.Tensor, eps: float = 1e-10) -> torch.Tensor:\n",
    "    \"\"\"Compute tr(sqrt(M)) for symmetric PSD M via eigenvalues.\n",
    "    Returns a scalar tensor that participates in autograd.\n",
    "    \"\"\"\n",
    "    Ms = 0.5 * (M + M.T)\n",
    "    evals = torch.linalg.eigvalsh(Ms)\n",
    "    evals = torch.clamp(evals, min=eps)\n",
    "    return torch.sqrt(evals).sum()\n",
    "\n",
    "\n",
    "def sinkhorn_project(P: torch.Tensor, row_sum: float, col_sum: float, iters: int = 10, eps: float = 1e-16) -> torch.Tensor:\n",
    "    \"\"\"Project P to the set {P >= 0, P 1 = row_sum, P^T 1 = col_sum} using multiplicative scaling.\n",
    "    Keeps autograd path (no .detach()).\n",
    "    \"\"\"\n",
    "    P = torch.clamp(P, min=0.0) + eps\n",
    "    for _ in range(iters):\n",
    "        # Scale columns to sum to row_sum (since P 1_col = row_sum for each column sum over rows)\n",
    "        col_sums = P.sum(dim=0, keepdim=True) + eps\n",
    "        P = P * (row_sum / col_sums)\n",
    "        # Scale rows to sum to col_sum (since P^T 1_row = col_sum for each row sum over cols)\n",
    "        row_sums = P.sum(dim=1, keepdim=True) + eps\n",
    "        P = P * (col_sum / row_sums)\n",
    "    return P\n",
    "\n",
    "\n",
    "def copt_objective_from_Ldag(Lx_dag: torch.Tensor, Ly_dag: torch.Tensor, P: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Compute the COPT closed-form objective (to be minimized) given Lx^dag, Ly^dag and current P.\n",
    "    Obj = M*tr(Lx^dag) + N*tr(Ly^dag) - 2 * tr( sqrt( (Ly^dag)^{1/2} P^T Lx^dag P (Ly^dag)^{1/2} ) )\n",
    "    \"\"\"\n",
    "    N = P.size(0)\n",
    "    M = P.size(1)\n",
    "    Ly_dag_sqrt = sqrtm_psd(Ly_dag)\n",
    "    # Z must be symmetric PSD\n",
    "    Z = Ly_dag_sqrt @ P.T @ Lx_dag @ P @ Ly_dag_sqrt\n",
    "    Z = 0.5 * (Z + Z.T)\n",
    "    term = trace_sqrt_psd(Z)\n",
    "    obj = M * torch.trace(Lx_dag) + N * torch.trace(Ly_dag) - 2.0 * term\n",
    "    return obj\n",
    "\n",
    "\n",
    "def copt_distance_from_laplacians(\n",
    "    Lx: torch.Tensor,\n",
    "    Ly: torch.Tensor,\n",
    "    steps: int = 300,\n",
    "    lr: float = 0.3,\n",
    "    sinkhorn_iters: int = 10,\n",
    "    seed: int | None = 0,\n",
    "    verbose: bool = True,\n",
    "):\n",
    "    \"\"\"Optimize P and return (distance_value, P_opt) for the COPT distance between two graphs.\n",
    "    Lx, Ly: Laplacians (N x N) and (M x M)\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        torch.manual_seed(seed)\n",
    "    device = Lx.device\n",
    "    N = Lx.size(0)\n",
    "    M = Ly.size(0)\n",
    "\n",
    "    # Precompute pseudoinverses (detached constants for speed/stability)\n",
    "    with torch.no_grad():\n",
    "        Lx_dag = pinv_laplacian(Lx).to(device)\n",
    "        Ly_dag = pinv_laplacian(Ly).to(device)\n",
    "\n",
    "    # Make them parameters for autograd? We treat them as constants.\n",
    "    Lx_dag = Lx_dag.requires_grad_(False)\n",
    "    Ly_dag = Ly_dag.requires_grad_(False)\n",
    "\n",
    "    # Initialize P with positive entries\n",
    "    P = torch.rand(N, M, device=device) + 1.0\n",
    "    P = sinkhorn_project(P, row_sum=N, col_sum=M, iters=30)\n",
    "    P = nn.Parameter(P)\n",
    "\n",
    "    opt = torch.optim.Adam([P], lr=lr)\n",
    "\n",
    "    best_val = None\n",
    "    best_P = None\n",
    "\n",
    "    for t in range(1, steps + 1):\n",
    "        opt.zero_grad()\n",
    "        # Re-project to feasible set before each step\n",
    "        with torch.no_grad():\n",
    "            P.data = sinkhorn_project(P.data, row_sum=N, col_sum=M, iters=sinkhorn_iters)\n",
    "        obj = copt_objective_from_Ldag(Lx_dag, Ly_dag, P)\n",
    "        obj.backward()\n",
    "        opt.step()\n",
    "\n",
    "        # Track best\n",
    "        val = obj.detach().item()\n",
    "        if best_val is None or val < best_val:\n",
    "            best_val = val\n",
    "            best_P = P.detach().clone()\n",
    "\n",
    "        if verbose and (t % max(1, steps // 10) == 0 or t == 1):\n",
    "            print(f\"[Step {t:4d}] COPT obj = {val:.6f}\")\n",
    "\n",
    "    # Final projection for cleanliness\n",
    "    with torch.no_grad():\n",
    "        best_P = sinkhorn_project(best_P, row_sum=N, col_sum=M, iters=50)\n",
    "    return best_val, best_P\n",
    "\n",
    "\n",
    "def copt_distance(\n",
    "    A_x: torch.Tensor,\n",
    "    A_y: torch.Tensor,\n",
    "    **kwargs,\n",
    "):\n",
    "    \"\"\"Convenience wrapper from adjacency to distance.\n",
    "    A_x: (N,N), A_y: (M,M) nonnegative symmetric (float) tensors.\n",
    "    Returns (distance_value, P_optimal)\n",
    "    \"\"\"\n",
    "    Lx = laplacian_from_adj(A_x)\n",
    "    Ly = laplacian_from_adj(A_y)\n",
    "    return copt_distance_from_laplacians(Lx, Ly, **kwargs)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Example Usage\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    torch.set_default_dtype(torch.float64)  # better numerical precision\n",
    "\n",
    "    # Build two small random undirected graphs (Erdos-Renyi) for demo\n",
    "    def er_graph(n, p, seed=0):\n",
    "        g = torch.rand(n, n)\n",
    "        g = torch.triu((g < p).double(), diagonal=1)\n",
    "        g = g + g.T\n",
    "        return g\n",
    "\n",
    "    N, M = 30, 25\n",
    "    A1 = er_graph(N, p=0.15, seed=0)\n",
    "    A2 = er_graph(M, p=0.20, seed=1)\n",
    "\n",
    "    dist, Popt = copt_distance(A1, A2, steps=200, lr=0.5, sinkhorn_iters=10, verbose=True)\n",
    "\n",
    "    print(\"\\nCOPT distance:\", dist)\n",
    "    print(\"P shape:\", Popt.shape)\n",
    "    print(\"Row sums (should be ~M):\", Popt.sum(dim=1)[:5])\n",
    "    print(\"Col sums (should be ~N):\", Popt.sum(dim=0)[:5])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
